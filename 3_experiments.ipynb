{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you submit an experiment, you use its run context to initialize and end the experiment run that is tracked in Azure Machine Learning, as shown in the following code sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment, Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Workspace.create(name='aml-workspace', subscription_id='3ed3266f-ff5e-4b56-b844-7568f3957f98', resource_group='am-rg')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws = Workspace.from_config()\n",
    "ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an experiment variable\n",
    "experiment = Experiment(\n",
    "    workspace=ws,\n",
    "    name=\"my-experiment\"\n",
    ")\n",
    "\n",
    "# start the experiment\n",
    "run = experiment.start_logging()\n",
    "# experiment goes here\n",
    "\n",
    "# end experiment\n",
    "run.complete()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging Metrics and Creating Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logging Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every experiment generates log files that include the messages that would be written to the terminal during interactive execution. This enables you to use simple print statements to write messages to the log. However, if you want to record named metrics for comparison across runs, you can do so by using the Run object; which provides a range of logging functions specifically for this purpose. These include:\n",
    "\n",
    "log: Record a single named value.\n",
    "\n",
    "log_list: Record a named list of values.\n",
    "\n",
    "log_row: Record a row with multiple columns.\n",
    "\n",
    "log_table: Record a dictionary as a table.\n",
    "\n",
    "log_image: Record an image file or a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an experiment variable\n",
    "experiment = Experiment(\n",
    "    workspace=ws,\n",
    "    name=\"my-experiment\"\n",
    ")\n",
    "\n",
    "# start the experiment\n",
    "run = experiment.start_logging()\n",
    "\n",
    "# load the dataset and count number of rows\n",
    "data = pd.read_csv(\"dummy_data.csv\", header=None)\n",
    "row_counts = len(data)\n",
    "\n",
    "# log the row count\n",
    "run.log('observation', row_counts)\n",
    "\n",
    "# complete the experiment\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retreiving and Viewing Logged Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view the metrics logged by an experiment run in Azure Machine Learning studio or by using the RunDetails widget in a notebook, as shown here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 1 - widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed1e1f689fd49ec8918ddf49996cb7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', 'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Completed\", \"workbench_run_details_uri\": \"https://ml.azure.com/runs/2f3fd509-6e81-47e7-acc8-1813e6222bf6?wsid=/subscriptions/3ed3266f-ff5e-4b56-b844-7568f3957f98/resourcegroups/am-rg/workspaces/aml-workspace&tid=fd50ea2b-9154-4926-9399-6cc1b0859c88\", \"run_id\": \"2f3fd509-6e81-47e7-acc8-1813e6222bf6\", \"run_properties\": {\"run_id\": \"2f3fd509-6e81-47e7-acc8-1813e6222bf6\", \"created_utc\": \"2022-09-02T08:27:16.900233Z\", \"properties\": {\"azureml.git.repository_uri\": \"https://github.com/mazumdarabhinav/azureml.git\", \"mlflow.source.git.repoURL\": \"https://github.com/mazumdarabhinav/azureml.git\", \"azureml.git.branch\": \"main\", \"mlflow.source.git.branch\": \"main\", \"azureml.git.commit\": \"53ea90cc1d920915bb4aa76f2dff5d273cab9faa\", \"mlflow.source.git.commit\": \"53ea90cc1d920915bb4aa76f2dff5d273cab9faa\", \"azureml.git.dirty\": \"True\", \"ContentSnapshotId\": \"ddd6ba56-fef6-4f4f-b477-d1927a7ba91a\"}, \"tags\": {}, \"end_time_utc\": \"2022-09-02T08:27:28.808468Z\", \"status\": \"Completed\", \"log_files\": {}, \"log_groups\": [], \"run_duration\": \"0:00:11\", \"run_number\": \"1662107236\", \"run_queued_details\": {\"status\": \"Completed\", \"details\": null}}, \"child_runs\": [], \"children_metrics\": {}, \"run_metrics\": [{\"name\": \"observation\", \"run_id\": \"2f3fd509-6e81-47e7-acc8-1813e6222bf6\", \"categories\": [0], \"series\": [{\"data\": [2]}]}], \"run_logs\": \"\\nRun is completed.\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.44.0\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"observation\": 2\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# get looged metrics\n",
    "metrics = run.get_metrics()\n",
    "print(json.dumps(metrics, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Output Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to logging metrics, an experiment can generate output files. Often these are trained machine learning models, but you can save any sort of file and make it available as an output of your experiment run. The output files of an experiment are saved in its outputs folder.\n",
    "\n",
    "The technique you use to add files to the outputs of an experiment depend on how you're running the experiment. The examples shown so far control the experiment lifecycle inline in your code, and when taking this approach you can upload local files to the run's outputs folder by using the Run object's upload_file method in your experiment code as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azureml._restclient.models.batch_artifact_content_information_dto.BatchArtifactContentInformationDto at 0x7f7aed5e0df0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.upload_file(name=\"outputs/samples.csv\", path_or_stream=\"./dummy_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running an experiment in a remote compute context (which we'll discuss later in this course), any files written to the outputs folder in the compute context are automatically uploaded to the run's outputs folder when the run completes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## retreiving output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  \"outputs/samples.csv\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "files = run.get_file_names()\n",
    "print(json.dumps(files, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a Script as an Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run an experiment inline using the start_logging method of the Experiment object, but it's more common to encapsulate the experiment logic in a script and run the script as an experiment. The script can be run in any valid compute context, making this a more flexible solution for running experiments as scale.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: my-experiment_1662108310_5a6273d5\n",
      "Web View: https://ml.azure.com/runs/my-experiment_1662108310_5a6273d5?wsid=/subscriptions/3ed3266f-ff5e-4b56-b844-7568f3957f98/resourcegroups/am-rg/workspaces/aml-workspace&tid=fd50ea2b-9154-4926-9399-6cc1b0859c88\n",
      "\n",
      "Streaming azureml-logs/60_control_log.txt\n",
      "=========================================\n",
      "\n",
      "[2022-09-02T08:45:13.896242] Using urllib.request Python 3.0 or later\n",
      "Streaming log file azureml-logs/60_control_log.txt\n",
      "Running: ['/bin/bash', '/private/var/folders/3g/fqf9w8vj3kn455_6g9l325kh0000gp/T/azureml_runs/my-experiment_1662108310_5a6273d5/azureml-environment-setup/conda_env_checker.sh']\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 65862\n",
      "Materialized conda environment not found on target: /Users/abhinavmazumdar/.azureml/envs/azureml_f3f7e6c5fb83d94df23933000bf02da3\n",
      "\n",
      "\n",
      "[2022-09-02T08:45:14.207969] Logging experiment preparation status in history service.\n",
      "Running: ['/bin/bash', '/private/var/folders/3g/fqf9w8vj3kn455_6g9l325kh0000gp/T/azureml_runs/my-experiment_1662108310_5a6273d5/azureml-environment-setup/conda_env_builder.sh']\n",
      "Running: ['conda', '--version']\n",
      "conda 4.12.0\n",
      "\n",
      "Creating conda environment...\n",
      "Running: ['conda', 'env', 'create', '-p', '/Users/abhinavmazumdar/.azureml/envs/azureml_f3f7e6c5fb83d94df23933000bf02da3', '-f', 'azureml-environment-setup/mutated_conda_dependencies.yml']\n",
      "Warning: you have pip-installed dependencies in your environment file, but you do not list pip itself as one of your conda dependencies.  Conda may not use the correct pip to install your packages, and they may end up in the wrong place.  Please add an explicit pip dependency.  I'm adding one for you, but still nagging you.\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.12.0\n",
      "  latest version: 4.14.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "libffi-3.3           | 48 KB     | ########## | 100% \n",
      "setuptools-65.3.0    | 782 KB    | ########## | 100% \n",
      "ncurses-6.3          | 1.1 MB    | ########## | 100% \n",
      "xz-5.2.6             | 233 KB    | ########## | 100% \n",
      "zlib-1.2.12          | 116 KB    | ########## | 100% \n",
      "readline-8.1.2       | 390 KB    | ########## | 100% \n",
      "libsqlite-3.39.2     | 897 KB    | ########## | 100% \n",
      "wheel-0.37.1         | 31 KB     | ########## | 100% \n",
      "pip-22.2.2           | 1.5 MB    | ########## | 100% \n",
      "libcxx-14.0.6        | 1.3 MB    | ########## | 100% \n",
      "python-3.8.13        | 12.9 MB   | ########## | 100% \n",
      "sqlite-3.39.2        | 898 KB    | ########## | 100% \n",
      "tk-8.6.12            | 3.4 MB    | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "\n",
      "[2022-09-02T08:48:11.586673] Entering context manager injector.\n",
      "[2022-09-02T08:48:13.972281] context_manager_injector.py Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError'], invocation=['4_experiment.py'])\n",
      "Script type = None\n",
      "[2022-09-02T08:48:13.975395] Entering Run History Context Manager.\n",
      "[2022-09-02T08:48:25.604856] Current directory: /private/var/folders/3g/fqf9w8vj3kn455_6g9l325kh0000gp/T/azureml_runs/my-experiment_1662108310_5a6273d5\n",
      "[2022-09-02T08:48:25.605664] Preparing to call script [4_experiment.py] with arguments:[]\n",
      "[2022-09-02T08:48:25.614393] After variable expansion, calling script [4_experiment.py] with arguments:[]\n",
      "\n",
      "\n",
      "\n",
      "[2022-09-02T08:48:25.628927] The experiment failed. Finalizing run...\n",
      "[2022-09-02T08:48:25.628966] Start FinalizingInRunHistory\n",
      "[2022-09-02T08:48:25.630451] Logging experiment finalizing status in history service.\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 66022\n",
      "Cleaning up all outstanding Run operations, waiting 300.0 seconds\n",
      "1 items cleaning up...\n",
      "Cleanup took 1.1127679347991943 seconds\n",
      "Traceback (most recent call last):\n",
      "  File \"4_experiment.py\", line 2, in <module>\n",
      "    import pandas as pd\n",
      "ModuleNotFoundError: No module named 'pandas'\n",
      "\n",
      "[2022-09-02T08:48:32.306685] Finished context manager injector with Exception.\n"
     ]
    },
    {
     "ename": "ExperimentExecutionException",
     "evalue": "ExperimentExecutionException:\n\tMessage: The output streaming for the run interrupted.\nBut the run is still executing on the compute target. \nDetails for canceling the run can be found here: https://aka.ms/aml-docs-cancel-run\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"The output streaming for the run interrupted.\\nBut the run is still executing on the compute target. \\nDetails for canceling the run can be found here: https://aka.ms/aml-docs-cancel-run\"\n    }\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/aml/lib/python3.9/site-packages/azureml/core/run.py:843\u001b[0m, in \u001b[0;36mRun.wait_for_completion\u001b[0;34m(self, show_output, wait_post_processing, raise_on_error)\u001b[0m\n\u001b[1;32m    842\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 843\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stream_run_output(\n\u001b[1;32m    844\u001b[0m         file_handle\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mstdout,\n\u001b[1;32m    845\u001b[0m         wait_post_processing\u001b[39m=\u001b[39;49mwait_post_processing,\n\u001b[1;32m    846\u001b[0m         raise_on_error\u001b[39m=\u001b[39;49mraise_on_error)\n\u001b[1;32m    847\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_details()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/aml/lib/python3.9/site-packages/azureml/core/run.py:1043\u001b[0m, in \u001b[0;36mRun._stream_run_output\u001b[0;34m(self, file_handle, wait_post_processing, raise_on_error)\u001b[0m\n\u001b[1;32m   1042\u001b[0m time\u001b[39m.\u001b[39msleep(Run\u001b[39m.\u001b[39m_wait_before_polling(time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m poll_start_time))\n\u001b[0;32m-> 1043\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_details \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_details()  \u001b[39m# TODO use FileWatcher\u001b[39;00m\n\u001b[1;32m   1045\u001b[0m \u001b[39m# Check whether there is a higher priority log than the one we are currently streaming (current_log)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/aml/lib/python3.9/site-packages/azureml/core/run.py:1259\u001b[0m, in \u001b[0;36mRun.get_details\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1194\u001b[0m \u001b[39m\"\"\"Get the definition, status information, current log files, and other details of the run.\u001b[39;00m\n\u001b[1;32m   1195\u001b[0m \n\u001b[1;32m   1196\u001b[0m \u001b[39m.. remarks::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1257\u001b[0m \u001b[39m:rtype: dict[str, str]\u001b[39;00m\n\u001b[1;32m   1258\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1259\u001b[0m details \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49mget_runstatus()\u001b[39m.\u001b[39mas_dict(key_transformer\u001b[39m=\u001b[39mcamel_case_transformer)\n\u001b[1;32m   1260\u001b[0m \u001b[39m# backfill common runtime logs as RH should no longer be proxying these\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/aml/lib/python3.9/site-packages/azureml/_run_impl/run_history_facade.py:194\u001b[0m, in \u001b[0;36mRunHistoryFacade.get_runstatus\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_runstatus\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 194\u001b[0m     run_status \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun\u001b[39m.\u001b[39;49mget_runstatus()\n\u001b[1;32m    195\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_cached_status(run_status\u001b[39m.\u001b[39mstatus)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/aml/lib/python3.9/site-packages/azureml/_restclient/run_client.py:107\u001b[0m, in \u001b[0;36mRunClient.get_runstatus\u001b[0;34m(self, caller, custom_headers, is_async)\u001b[0m\n\u001b[1;32m    104\u001b[0m kwargs \u001b[39m=\u001b[39m _generate_client_kwargs(\n\u001b[1;32m    105\u001b[0m     is_async\u001b[39m=\u001b[39mis_async, caller\u001b[39m=\u001b[39mcaller, custom_headers\u001b[39m=\u001b[39mcustom_headers)\n\u001b[0;32m--> 107\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_with_run_arguments(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49mrun\u001b[39m.\u001b[39;49mget_details, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/aml/lib/python3.9/site-packages/azureml/_restclient/run_client.py:564\u001b[0m, in \u001b[0;36mRunClient._execute_with_run_arguments\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_execute_with_run_arguments\u001b[39m(\u001b[39mself\u001b[39m, func, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 564\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_with_arguments(func, copy\u001b[39m.\u001b[39;49mdeepcopy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_arguments), \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/aml/lib/python3.9/site-packages/azureml/_restclient/clientbase.py:589\u001b[0m, in \u001b[0;36mClientBase._execute_with_arguments\u001b[0;34m(self, func, args_list, *args, **kwargs)\u001b[0m\n\u001b[1;32m    588\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 589\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_api(func, \u001b[39m*\u001b[39;49margs_list, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    590\u001b[0m \u001b[39mexcept\u001b[39;00m ErrorResponseException \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/aml/lib/python3.9/site-packages/azureml/_restclient/clientbase.py:245\u001b[0m, in \u001b[0;36mClientBase._call_api\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 245\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_with_base_arguments(func, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/aml/lib/python3.9/site-packages/azureml/_restclient/clientbase.py:333\u001b[0m, in \u001b[0;36mClientBase._execute_with_base_arguments\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m total_retry \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretries \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretries\n\u001b[0;32m--> 333\u001b[0m \u001b[39mreturn\u001b[39;00m ClientBase\u001b[39m.\u001b[39;49m_execute_func_internal(\n\u001b[1;32m    334\u001b[0m     back_off, total_retry, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_logger, func, _noop_reset, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/aml/lib/python3.9/site-packages/azureml/_restclient/clientbase.py:358\u001b[0m, in \u001b[0;36mClientBase._execute_func_internal\u001b[0;34m(cls, back_off, total_retry, logger, func, reset_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mClientBase: Calling \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m with url \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(func_name, func_url))\n\u001b[0;32m--> 358\u001b[0m response \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    359\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39misinstance\u001b[39m(response, Response) \u001b[39mand\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_is_retryable_status_code(response\u001b[39m.\u001b[39mstatus_code)\n\u001b[1;32m    360\u001b[0m         \u001b[39mand\u001b[39;00m left_retry \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[1;32m    361\u001b[0m     \u001b[39m# This is the handle the error case 1. response.raise_for_status only throws HTTPError exception.\u001b[39;00m\n\u001b[1;32m    362\u001b[0m     \u001b[39m# if the status_code is retryable and it is not the last retry, then the exception is thrown.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m     \u001b[39m# Otherwise, we will return the response directly.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/aml/lib/python3.9/site-packages/azureml/_restclient/operations/run_operations.py:299\u001b[0m, in \u001b[0;36mRunOperations.get_details\u001b[0;34m(self, subscription_id, resource_group_name, workspace_name, experiment_name, run_id, custom_headers, raw, **operation_config)\u001b[0m\n\u001b[1;32m    298\u001b[0m request \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39mget(url, query_parameters)\n\u001b[0;32m--> 299\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49msend(request, header_parameters, stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moperation_config)\n\u001b[1;32m    301\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m200\u001b[39m]:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/aml/lib/python3.9/site-packages/msrest/service_client.py:336\u001b[0m, in \u001b[0;36mServiceClient.send\u001b[0;34m(self, request, headers, content, **kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 336\u001b[0m     pipeline_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mpipeline\u001b[39m.\u001b[39;49mrun(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    337\u001b[0m     \u001b[39m# There is too much thing that expects this method to return a \"requests.Response\"\u001b[39;00m\n\u001b[1;32m    338\u001b[0m     \u001b[39m# to break it in a compatible release.\u001b[39;00m\n\u001b[1;32m    339\u001b[0m     \u001b[39m# Also, to be pragmatic in the \"sync\" world \"requests\" rules anyway.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m     \u001b[39m# However, attach the Universal HTTP response\u001b[39;00m\n\u001b[1;32m    341\u001b[0m     \u001b[39m# to get the streaming generator.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/aml/lib/python3.9/site-packages/msrest/pipeline/__init__.py:197\u001b[0m, in \u001b[0;36mPipeline.run\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m first_node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_impl_policies[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_impl_policies \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sender\n\u001b[0;32m--> 197\u001b[0m \u001b[39mreturn\u001b[39;00m first_node\u001b[39m.\u001b[39;49msend(pipeline_request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/aml/lib/python3.9/site-packages/msrest/pipeline/__init__.py:150\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/aml/lib/python3.9/site-packages/msrest/pipeline/requests.py:65\u001b[0m, in \u001b[0;36mRequestsCredentialsPolicy.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_creds\u001b[39m.\u001b[39;49msigned_session(session)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m: \u001b[39m# Credentials does not support session injection\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/aml/lib/python3.9/site-packages/azureml/core/authentication.py:251\u001b[0m, in \u001b[0;36mAbstractAuthentication.signed_session\u001b[0;34m(self, session)\u001b[0m\n\u001b[1;32m    250\u001b[0m session \u001b[39m=\u001b[39m session \u001b[39mor\u001b[39;00m Session()\n\u001b[0;32m--> 251\u001b[0m session\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_authentication_header())\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m session\n",
      "File \u001b[0;32m/opt/anaconda3/envs/aml/lib/python3.9/site-packages/azureml/core/authentication.py:128\u001b[0m, in \u001b[0;36mAbstractAuthentication.get_authentication_header\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39m# We return a new dictionary each time, as some functions modify the headers returned\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[39m# by this function.\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m auth_header \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mAuthorization\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mBearer \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_arm_token()}\n\u001b[1;32m    129\u001b[0m \u001b[39mreturn\u001b[39;00m auth_header\n",
      "File \u001b[0;32m/opt/anaconda3/envs/aml/lib/python3.9/site-packages/azureml/core/authentication.py:386\u001b[0m, in \u001b[0;36m_login_on_failure_decorator.<locals>.actual_decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    385\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 386\u001b[0m lock_to_use\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    387\u001b[0m duration \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mExperimentExecutionException\u001b[0m              Traceback (most recent call last)",
      "\u001b[1;32m/Users/abhinavmazumdar/workspace/azure_contents/azureml/3_experiments.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/abhinavmazumdar/workspace/azure_contents/azureml/3_experiments.ipynb#X60sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m experiment \u001b[39m=\u001b[39m Experiment(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/abhinavmazumdar/workspace/azure_contents/azureml/3_experiments.ipynb#X60sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     workspace\u001b[39m=\u001b[39mws,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhinavmazumdar/workspace/azure_contents/azureml/3_experiments.ipynb#X60sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmy-experiment\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhinavmazumdar/workspace/azure_contents/azureml/3_experiments.ipynb#X60sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhinavmazumdar/workspace/azure_contents/azureml/3_experiments.ipynb#X60sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m run \u001b[39m=\u001b[39m experiment\u001b[39m.\u001b[39msubmit(config\u001b[39m=\u001b[39mscript_config)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/abhinavmazumdar/workspace/azure_contents/azureml/3_experiments.ipynb#X60sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m run\u001b[39m.\u001b[39;49mwait_for_completion(show_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/aml/lib/python3.9/site-packages/azureml/core/run.py:854\u001b[0m, in \u001b[0;36mRun.wait_for_completion\u001b[0;34m(self, show_output, wait_post_processing, raise_on_error)\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    849\u001b[0m         error_message \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mThe output streaming for the run interrupted.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \\\n\u001b[1;32m    850\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mBut the run is still executing on the compute target. \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \\\n\u001b[1;32m    851\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mDetails for canceling the run can be found here: \u001b[39m\u001b[39m\"\u001b[39m \\\n\u001b[1;32m    852\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mhttps://aka.ms/aml-docs-cancel-run\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 854\u001b[0m         \u001b[39mraise\u001b[39;00m ExperimentExecutionException(error_message)\n\u001b[1;32m    855\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    856\u001b[0m     running_states \u001b[39m=\u001b[39m RUNNING_STATES\n",
      "\u001b[0;31mExperimentExecutionException\u001b[0m: ExperimentExecutionException:\n\tMessage: The output streaming for the run interrupted.\nBut the run is still executing on the compute target. \nDetails for canceling the run can be found here: https://aka.ms/aml-docs-cancel-run\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"The output streaming for the run interrupted.\\nBut the run is still executing on the compute target. \\nDetails for canceling the run can be found here: https://aka.ms/aml-docs-cancel-run\"\n    }\n}"
     ]
    }
   ],
   "source": [
    "from azureml.core import Experiment, ScriptRunConfig\n",
    "\n",
    "# create a script config\n",
    "script_config = ScriptRunConfig(source_directory=\".\",\n",
    "script='4_experiment.py')\n",
    "\n",
    "# submitt the experiment\n",
    "experiment = Experiment(\n",
    "    workspace=ws,\n",
    "    name=\"my-experiment\"\n",
    ")\n",
    "\n",
    "run = experiment.submit(config=script_config)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('aml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "53205f1a5bceb014cede6b4cb20e924585cbbcf89cb486efb59ab5521987e362"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
